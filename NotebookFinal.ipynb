{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36682add",
   "metadata": {},
   "source": [
    "### Case Study Groupe 10\n",
    "Arne Herlinghaus, Max Lütkemeyer, Thomas Mogos, Tim Strauss, Simon Luttmann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110cfe0",
   "metadata": {},
   "source": [
    "# Final Model creation Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5b85a",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:54:57.996812Z",
     "start_time": "2025-05-20T16:54:57.766826Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fdc70",
   "metadata": {},
   "source": [
    "#### Load original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1c757215853f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:54:58.143539Z",
     "start_time": "2025-05-20T16:54:58.002253Z"
    }
   },
   "outputs": [],
   "source": [
    "train_csv = \"data_6_channels_train.csv\" # ← Pfad ggf. anpassen\n",
    "df = (pd.read_csv(train_csv)\n",
    "      .rename(columns={\"numerical_id\": \"forest_id\",\n",
    "                       \"class\": \"is_disturbance\",\n",
    "                       \"BLU\": \"blue\", \"GRN\": \"green\", \"RED\": \"red\",\n",
    "                       \"NIR\": \"near_infrared\",\n",
    "                       \"SW1\": \"shortwave_infrared_1\", \"SW2\": \"shortwave_infrared_2\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06500cfd",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680c708c5cf4a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:54:58.240544Z",
     "start_time": "2025-05-20T16:54:58.234439Z"
    }
   },
   "outputs": [],
   "source": [
    "def engineer_features(group: pd.DataFrame, lags: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Spektrale Indizes + Landsat-8 TCT + 3-Jahres-Stats + Lags/Deltas + Raum/Zeit.\"\"\"\n",
    "    eps = 1e-6\n",
    "    g = group.copy()\n",
    "\n",
    "    # ---------- Basis-Indizes ----------------------------------------\n",
    "    g[\"NDVI\"] = (g.near_infrared - g.red) / (g.near_infrared + g.red + eps)\n",
    "    g[\"NDMI\"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)\n",
    "    g[\"NDWI\"] = (g.green - g.near_infrared) / (g.green + g.near_infrared + eps)\n",
    "    g[\"NBR\"] = (g.near_infrared - g.shortwave_infrared_2) / (g.near_infrared + g.shortwave_infrared_2 + eps)\n",
    "    g[\"EVI\"] = 2.5 * (g.near_infrared - g.red) / (g.near_infrared + 6 * g.red - 7.5 * g.blue + 1 + eps)\n",
    "    g[\"NBR2\"] = (g.shortwave_infrared_1 - g.shortwave_infrared_2) / (\n",
    "                g.shortwave_infrared_1 + g.shortwave_infrared_2 + eps)\n",
    "    \n",
    "    # ---------- Trockenheits-Ratio -----------------------------------\n",
    "    g[\"SWIR_ratio\"] = g.shortwave_infrared_2 / (g.shortwave_infrared_1 + eps)\n",
    "\n",
    "    # ---------- Landsat-8 Tasseled-Cap (Baig 2014) -------------------\n",
    "    b2, b3, b4 = g.blue, g.green, g.red\n",
    "    b5, b6, b7 = g.near_infrared, g.shortwave_infrared_1, g.shortwave_infrared_2\n",
    "    g[\"TCB\"] = 0.3029 * b2 + 0.2786 * b3 + 0.4733 * b4 + 0.5599 * b5 + 0.5080 * b6 + 0.1872 * b7\n",
    "    g[\"TCG\"] = -0.2941 * b2 - 0.2430 * b3 - 0.5424 * b4 + 0.7276 * b5 + 0.0713 * b6 - 0.1608 * b7\n",
    "    g[\"TCW\"] = 0.1511 * b2 + 0.1973 * b3 + 0.3283 * b4 + 0.3407 * b5 - 0.7117 * b6 - 0.4559 * b7\n",
    "    g[\"TCT4\"] = -0.8239 * b2 + 0.0849 * b3 + 0.4396 * b4 - 0.0580 * b5 + 0.2013 * b6 - 0.2773 * b7\n",
    "    g[\"TCT5\"] = -0.3294 * b2 + 0.0557 * b3 + 0.1056 * b4 + 0.1855 * b5 - 0.4349 * b6 + 0.8085 * b7\n",
    "    g[\"TCT6\"] = 0.1079 * b2 - 0.9023 * b3 + 0.4119 * b4 + 0.0575 * b5 - 0.0259 * b6 + 0.0252 * b7\n",
    "\n",
    "    # --- Interaktions-Features --------------------------------------\n",
    "    g[\"TCBxTCG\"] = g.TCB * g.TCG\n",
    "    g[\"TCBxTCW\"] = g.TCB * g.TCW\n",
    "    g[\"TCBxNBR\"] = g.TCB * g.NBR\n",
    "\n",
    "    # ---------- 3-Jahres-Median, Std, Anomalie (NDVI & NBR) ----------\n",
    "    for idx in (\"NDVI\", \"NBR\"):\n",
    "        g[f\"{idx}_med3\"] = g[idx].rolling(3, min_periods=2).median()\n",
    "        g[f\"{idx}_std3\"] = g[idx].rolling(3, min_periods=2).std()\n",
    "        g[f\"{idx}_anom\"] = (g[idx] - g[f\"{idx}_med3\"]) / (g[f\"{idx}_med3\"].abs() + eps)\n",
    "\n",
    "    # ---------- Lags & Deltas (blockweise, effizient) ---------------\n",
    "    base_cols = [\n",
    "        \"NDVI\", \"NDMI\", \"NDWI\", \"NBR\", \"EVI\", \"NBR2\", \"SWIR_ratio\",\n",
    "        \"TCB\", \"TCG\", \"TCW\", \"TCT4\", \"TCT5\", \"TCT6\",\n",
    "        \"TCBxTCG\", \"TCBxTCW\", \"TCBxNBR\",\n",
    "        \"blue\", \"green\", \"red\", \"near_infrared\", \"shortwave_infrared_1\", \"shortwave_infrared_2\"\n",
    "    ]\n",
    "\n",
    "    lag_features = []\n",
    "    for lag in range(1, lags + 1):\n",
    "        shifted = g[base_cols].shift(lag).rename(columns=lambda col: f\"l{lag}_{col}\")\n",
    "        deltas = (g[base_cols] - g[base_cols].shift(lag)).rename(columns=lambda col: f\"d{lag}_{col}\")\n",
    "        lag_features.extend([shifted, deltas])\n",
    "    g = pd.concat([g] + lag_features, axis=1)\n",
    "\n",
    "    # ---------- Raum- & Zeit-Features --------------------------------\n",
    "    g[\"year_sin\"] = np.sin(2 * np.pi * g.year / 10)\n",
    "    g[\"year_cos\"] = np.cos(2 * np.pi * g.year / 10)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01be04c94dbece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:55:04.518837Z",
     "start_time": "2025-05-20T16:54:58.280689Z"
    }
   },
   "outputs": [],
   "source": [
    "ft_file = \"all_data_all_features.csv\"\n",
    "\n",
    "if not Path(ft_file).exists():\n",
    "    feats = (df.groupby(\"forest_id\")\n",
    "             .apply(engineer_features)\n",
    "             .fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "             .reset_index(drop=True))\n",
    "    feats.to_csv(ft_file, index=False)\n",
    "else:\n",
    "    feats = pd.read_csv(ft_file)\n",
    "\n",
    "feature_cols = [c for c in feats.columns\n",
    "                if c not in (\"forest_id\", \"year\", \"is_disturbance\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbaf87d",
   "metadata": {},
   "source": [
    "#### Test|Validation 80|20 Split with Groups by forest_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf66f5463793c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:55:04.896705Z",
     "start_time": "2025-05-20T16:55:04.529475Z"
    }
   },
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "tr_idx, va_idx = next(gss.split(feats, groups=feats[\"forest_id\"]))\n",
    "\n",
    "train_df, val_df = feats.iloc[tr_idx], feats.iloc[va_idx]\n",
    "X_train, y_train = train_df[feature_cols].values, train_df[\"is_disturbance\"].values\n",
    "X_val, y_val = val_df[feature_cols].values, val_df[\"is_disturbance\"].values\n",
    "print(f\"Train {len(train_df):,} | Val {len(val_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953769c5",
   "metadata": {},
   "source": [
    "# ASK TIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835cdf6",
   "metadata": {},
   "source": [
    "#### Train an XGBoost Ensemble with GroupKFolds based on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ecc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: DESCRIPTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0290ccd90ec7ad",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-20T16:55:04.916945Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "seeds = [42, 1337, 2025, 7, 99]\n",
    "pos_weights = [1.0, 1.05, 1.1, 1.15]\n",
    "base_params = dict(objective=\"binary:logistic\",\n",
    "                   eval_metric=\"aucpr\",\n",
    "                   eta=0.05,\n",
    "                   max_depth=7,\n",
    "                   subsample=0.8,\n",
    "                   colsample_bytree=0.7,\n",
    "                   min_child_weight=2,\n",
    "                   gamma=0.3,\n",
    "                   alpha=0.7,\n",
    "                   n_estimators=2000,\n",
    "                   tree_method=\"hist\",\n",
    "                   n_jobs=-1,\n",
    "                   early_stopping_rounds=60)\n",
    "\n",
    "groups = train_df[\"forest_id\"].values\n",
    "oof_proba = np.zeros(len(train_df))\n",
    "for fold, (i_tr, i_va) in enumerate(GroupKFold(5).split(X_train, y_train, groups), 1): # K fold based on forest_id\n",
    "    X_tr, y_tr = X_train[i_tr], y_train[i_tr]\n",
    "    X_va, y_va = X_train[i_va], y_train[i_va]\n",
    "    best, f1_b = np.zeros(len(i_va)), -1\n",
    "    \n",
    "    for w in pos_weights:\n",
    "        proba = np.zeros(len(i_va))\n",
    "\n",
    "        for s in seeds:\n",
    "            p = base_params | {\"random_state\": s,\n",
    "                               \"scale_pos_weight\": w * (y_tr == 0).sum() / (y_tr == 1).sum()}# adjust scaling of positive instances for the XGBoost model training based on the class imbalance\n",
    "            bst = XGBClassifier(**p).fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "            iso = CalibratedClassifierCV(bst, cv=\"prefit\", method=\"isotonic\").fit(X_va, y_va)# isotonic regression for better probability estimates\n",
    "            proba += iso.predict_proba(X_va)[:, 1]\n",
    "            \n",
    "        proba /= len(seeds)\n",
    "        f1_tmp = f1_score(y_va, (proba >= 0.5))\n",
    "        if f1_tmp > f1_b: best, f1_b = proba, f1_tmp\n",
    "    oof_proba[i_va] = best\n",
    "prec, rec, thr = precision_recall_curve(y_train, oof_proba)\n",
    "tau_star = float(thr[np.argmax(2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-9))])\n",
    "print(f\"Optimal τ* = {tau_star:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2eea8",
   "metadata": {},
   "source": [
    "We choose the XGBoost Ensemble model because it made the best predictions on our validation set compared to other Models like XGBoost, RandomForest, ExtraTrees, Logistic Regression, KNeighrest Neighboor and SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032f3c4",
   "metadata": {},
   "source": [
    "#### Retrain xGboost Ensemble Model based on all original forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c74bde33ce229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:54:53.802808Z",
     "start_time": "2025-05-20T15:38:40.653251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "TRAIN FULL ENSEMBLE & SAVE\n",
      "══════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/PycharmProjects/forest_disturbance_detection/.venv/lib/python3.12/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ensemble (5 Modelle) gespeichert → models/xgb_iso_ensemble.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "full_X = feats[feature_cols].values\n",
    "full_y = feats[\"is_disturbance\"].values\n",
    "pos_w  = (full_y == 0).sum() / (full_y == 1).sum()\n",
    "\n",
    "ensemble = []\n",
    "for s in seeds:\n",
    "    params = base_params.copy()\n",
    "    params.update({\n",
    "        \"random_state\"          : s,\n",
    "        \"scale_pos_weight\"      : pos_w,\n",
    "        \"early_stopping_rounds\" : None\n",
    "    })\n",
    "\n",
    "    bst = XGBClassifier(**params).fit(full_X, full_y, verbose=False)\n",
    "\n",
    "    iso = CalibratedClassifierCV(bst, cv=\"prefit\", method=\"isotonic\")\n",
    "    iso.fit(full_X, full_y)\n",
    "\n",
    "    ensemble.append(iso)\n",
    "\n",
    "artifact = {\n",
    "    \"models\"      : ensemble,\n",
    "    \"threshold\"   : tau_star,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"params\"      : params,\n",
    "    \"timestamp\"   : datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with open(MODEL_DIR / \"xgb_iso_ensemble1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artifact, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"✓ Ensemble ({len(ensemble)} Modelle) gespeichert → models/xgb_iso_ensemble.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ef0452d77de338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:54:53.804648Z",
     "start_time": "2025-05-20T15:48:13.486089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ensemble (5 Modelle) gespeichert → models/xgb_iso_ensemble.pkl\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with open(MODEL_DIR / \"xgb_iso_ensemble1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artifact, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"✓ Ensemble ({len(ensemble)} Modelle) gespeichert → models/xgb_iso_ensemble.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdbbe4",
   "metadata": {},
   "source": [
    "#### Inferenz on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90ed93ade748fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:54:53.805046Z",
     "start_time": "2025-05-20T16:22:42.781094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════════════\n",
      "INFERENCE & MERGE WITH ORIGINAL CSV\n",
      "══════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/s6j5yq4s12j6wvms4084h50h0000gn/T/ipykernel_26023/1338670823.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(engineer_features)\n",
      "/var/folders/9l/s6j5yq4s12j6wvms4084h50h0000gn/T/ipykernel_26023/1338670823.py:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  .fillna(method=\"ffill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 'data_6_channels_test_pub_with_predictions.csv' geschrieben – 113,424 Zeilen\n",
      "Label-Verteilung: 0 = 112,951  |  1 = 473\n"
     ]
    }
   ],
   "source": [
    "TEST_CSV = \"data_6_channels_test_pub.csv\"\n",
    "OUT_CSV = \"data_6_channels_test_pub_with_predictions.csv\"\n",
    "MODEL_PKL = \"models/xgb_iso_ensemble.pkl\"\n",
    "\n",
    "# 6.1  Original-CSV laden (ohne Spaltenänderung merken)\n",
    "orig_test = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# 6.2  Für das Modell temporär umbenennen\n",
    "tmp = (orig_test\n",
    "       .rename(columns={\n",
    "    \"numerical_id\": \"forest_id\",\n",
    "    \"BLU\": \"blue\", \"GRN\": \"green\", \"RED\": \"red\",\n",
    "    \"NIR\": \"near_infrared\", \"SW1\": \"shortwave_infrared_1\",\n",
    "    \"SW2\": \"shortwave_infrared_2\"})\n",
    "       .copy())\n",
    "\n",
    "# Zeilen-/Spalten-Koordinaten für Feature-Engineering\n",
    "n_cols = 142\n",
    "tmp[\"row\"] = tmp[\"forest_id\"] // n_cols\n",
    "tmp[\"col\"] = tmp[\"forest_id\"] % n_cols\n",
    "\n",
    "# 6.3  Feature-Engineering\n",
    "test_feat = (tmp.groupby(\"forest_id\")\n",
    "             .apply(engineer_features)\n",
    "             .fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "# 6.4  Ensemble + Threshold laden\n",
    "with open(MODEL_PKL, \"rb\") as f:\n",
    "    art = pickle.load(f)\n",
    "\n",
    "X_test = test_feat[art[\"feature_cols\"]].values\n",
    "proba = sum(m.predict_proba(X_test)[:, 1] for m in art[\"models\"]) / len(art[\"models\"])\n",
    "pred = (proba >= art[\"threshold\"]).astype(int)\n",
    "\n",
    "# 6.5  Prediction-DataFrame zum Mergen vorbereiten\n",
    "pred_df = (test_feat[[\"forest_id\", \"year\"]]\n",
    "           .assign(is_disturbance=pred)\n",
    "           .rename(columns={\"forest_id\": \"numerical_id\"}))\n",
    "\n",
    "# 6.6  Mit Original-CSV zusammenführen  (inner-merge garantiert 1-zu-1)\n",
    "merged = (orig_test\n",
    "          .merge(pred_df, on=[\"numerical_id\", \"year\"], how=\"left\"))\n",
    "\n",
    "# 6.7  Spaltenreihenfolge (wie Bild + neue Spalte)\n",
    "col_order = [\"fid\", \"year\", \"numerical_id\",\n",
    "             \"BLU\", \"GRN\", \"RED\", \"NIR\", \"SW1\", \"SW2\",\n",
    "             \"is_disturbance\"]\n",
    "merged = merged[col_order]\n",
    "\n",
    "# 6.8  Datei schreiben\n",
    "merged.to_csv(OUT_CSV, index=False)\n",
    "print(f\"✓ '{OUT_CSV}' geschrieben – {merged.shape[0]:,} Zeilen\")\n",
    "\n",
    "# 6.9  Kurzer Überblick\n",
    "print(f\"Label-Verteilung: 0 = {(merged.is_disturbance == 0).sum():,}  |  \"\n",
    "      f\"1 = {(merged.is_disturbance == 1).sum():,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

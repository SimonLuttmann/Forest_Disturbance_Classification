{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36682add",
   "metadata": {},
   "source": [
    "### Case Study Groupe 10\n",
    "Arne Herlinghaus, Max Lütkemeyer, Thomas Mogos, Tim Strauss, Simon Luttmann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110cfe0",
   "metadata": {},
   "source": [
    "# Final Model creation Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5b85a",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:14:21.359691Z",
     "start_time": "2025-05-20T20:14:21.355760Z"
    }
   },
   "source": [
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "585fdc70",
   "metadata": {},
   "source": [
    "#### Load original data"
   ]
  },
  {
   "cell_type": "code",
   "id": "dfe1c757215853f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:14:21.648062Z",
     "start_time": "2025-05-20T20:14:21.370315Z"
    }
   },
   "source": [
    "train_csv = \"data_6_channels_train.csv\" # ← Pfad ggf. anpassen\n",
    "df = (pd.read_csv(train_csv)\n",
    "      .rename(columns={\"numerical_id\": \"forest_id\",\n",
    "                       \"class\": \"is_disturbance\",\n",
    "                       \"BLU\": \"blue\", \"GRN\": \"green\", \"RED\": \"red\",\n",
    "                       \"NIR\": \"near_infrared\",\n",
    "                       \"SW1\": \"shortwave_infrared_1\", \"SW2\": \"shortwave_infrared_2\"}))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "06500cfd",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "id": "6680c708c5cf4a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:14:21.670574Z",
     "start_time": "2025-05-20T20:14:21.660712Z"
    }
   },
   "source": [
    "def engineer_features(group: pd.DataFrame, lags: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Spektrale Indizes + Landsat-8 TCT + 3-Jahres-Stats + Lags/Deltas + Raum/Zeit.\"\"\"\n",
    "    eps = 1e-6\n",
    "    g = group.copy()\n",
    "\n",
    "    # ---------- Basis-Indizes ----------------------------------------\n",
    "    g[\"NDVI\"] = (g.near_infrared - g.red) / (g.near_infrared + g.red + eps)\n",
    "    g[\"NDMI\"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)\n",
    "    g[\"NDWI\"] = (g.green - g.near_infrared) / (g.green + g.near_infrared + eps)\n",
    "    g[\"NBR\"] = (g.near_infrared - g.shortwave_infrared_2) / (g.near_infrared + g.shortwave_infrared_2 + eps)\n",
    "    g[\"EVI\"] = 2.5 * (g.near_infrared - g.red) / (g.near_infrared + 6 * g.red - 7.5 * g.blue + 1 + eps)\n",
    "    g[\"NBR2\"] = (g.shortwave_infrared_1 - g.shortwave_infrared_2) / (\n",
    "                g.shortwave_infrared_1 + g.shortwave_infrared_2 + eps)\n",
    "    \n",
    "    # ---------- Trockenheits-Ratio -----------------------------------\n",
    "    g[\"SWIR_ratio\"] = g.shortwave_infrared_2 / (g.shortwave_infrared_1 + eps)\n",
    "\n",
    "    # ---------- Landsat-8 Tasseled-Cap (Baig 2014) -------------------\n",
    "    b2, b3, b4 = g.blue, g.green, g.red\n",
    "    b5, b6, b7 = g.near_infrared, g.shortwave_infrared_1, g.shortwave_infrared_2\n",
    "    g[\"TCB\"] = 0.3029 * b2 + 0.2786 * b3 + 0.4733 * b4 + 0.5599 * b5 + 0.5080 * b6 + 0.1872 * b7\n",
    "    g[\"TCG\"] = -0.2941 * b2 - 0.2430 * b3 - 0.5424 * b4 + 0.7276 * b5 + 0.0713 * b6 - 0.1608 * b7\n",
    "    g[\"TCW\"] = 0.1511 * b2 + 0.1973 * b3 + 0.3283 * b4 + 0.3407 * b5 - 0.7117 * b6 - 0.4559 * b7\n",
    "    g[\"TCT4\"] = -0.8239 * b2 + 0.0849 * b3 + 0.4396 * b4 - 0.0580 * b5 + 0.2013 * b6 - 0.2773 * b7\n",
    "    g[\"TCT5\"] = -0.3294 * b2 + 0.0557 * b3 + 0.1056 * b4 + 0.1855 * b5 - 0.4349 * b6 + 0.8085 * b7\n",
    "    g[\"TCT6\"] = 0.1079 * b2 - 0.9023 * b3 + 0.4119 * b4 + 0.0575 * b5 - 0.0259 * b6 + 0.0252 * b7\n",
    "\n",
    "    # --- Interaktions-Features --------------------------------------\n",
    "    g[\"TCBxTCG\"] = g.TCB * g.TCG\n",
    "    g[\"TCBxTCW\"] = g.TCB * g.TCW\n",
    "    g[\"TCBxNBR\"] = g.TCB * g.NBR\n",
    "\n",
    "    # ---------- 3-Jahres-Median, Std, Anomalie (NDVI & NBR) ----------\n",
    "    for idx in (\"NDVI\", \"NBR\"):\n",
    "        g[f\"{idx}_med3\"] = g[idx].rolling(3, min_periods=2).median()\n",
    "        g[f\"{idx}_std3\"] = g[idx].rolling(3, min_periods=2).std()\n",
    "        g[f\"{idx}_anom\"] = (g[idx] - g[f\"{idx}_med3\"]) / (g[f\"{idx}_med3\"].abs() + eps)\n",
    "\n",
    "    # ---------- Lags & Deltas (blockweise, effizient) ---------------\n",
    "    base_cols = [\n",
    "        \"NDVI\", \"NDMI\", \"NDWI\", \"NBR\", \"EVI\", \"NBR2\", \"SWIR_ratio\",\n",
    "        \"TCB\", \"TCG\", \"TCW\", \"TCT4\", \"TCT5\", \"TCT6\",\n",
    "        \"TCBxTCG\", \"TCBxTCW\", \"TCBxNBR\",\n",
    "        \"blue\", \"green\", \"red\", \"near_infrared\", \"shortwave_infrared_1\", \"shortwave_infrared_2\"\n",
    "    ]\n",
    "\n",
    "    lag_features = []\n",
    "    for lag in range(1, lags + 1):\n",
    "        shifted = g[base_cols].shift(lag).rename(columns=lambda col: f\"l{lag}_{col}\")\n",
    "        deltas = (g[base_cols] - g[base_cols].shift(lag)).rename(columns=lambda col: f\"d{lag}_{col}\")\n",
    "        lag_features.extend([shifted, deltas])\n",
    "    g = pd.concat([g] + lag_features, axis=1)\n",
    "\n",
    "    # ---------- Raum- & Zeit-Features --------------------------------\n",
    "    g[\"year_sin\"] = np.sin(2 * np.pi * g.year / 10)\n",
    "    g[\"year_cos\"] = np.cos(2 * np.pi * g.year / 10)\n",
    "\n",
    "    return g"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "db01be04c94dbece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:14:29.334272Z",
     "start_time": "2025-05-20T20:14:21.695788Z"
    }
   },
   "source": [
    "ft_file = \"all_data_all_features.csv\"\n",
    "\n",
    "if not Path(ft_file).exists():\n",
    "    feats = (df.groupby(\"forest_id\")\n",
    "             .apply(engineer_features)\n",
    "             .fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "             .reset_index(drop=True))\n",
    "    feats.to_csv(ft_file, index=False)\n",
    "else:\n",
    "    feats = pd.read_csv(ft_file)\n",
    "\n",
    "feature_cols = [c for c in feats.columns\n",
    "                if c not in (\"forest_id\", \"year\", \"is_disturbance\")]\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "ddbaf87d",
   "metadata": {},
   "source": [
    "#### Test|Validation 80|20 Split with Groups by forest_id"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8bf66f5463793c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:14:29.871453Z",
     "start_time": "2025-05-20T20:14:29.348164Z"
    }
   },
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "tr_idx, va_idx = next(gss.split(feats, groups=feats[\"forest_id\"]))\n",
    "\n",
    "train_df, val_df = feats.iloc[tr_idx], feats.iloc[va_idx]\n",
    "X_train, y_train = train_df[feature_cols].values, train_df[\"is_disturbance\"].values\n",
    "X_val, y_val = val_df[feature_cols].values, val_df[\"is_disturbance\"].values\n",
    "print(f\"Train {len(train_df):,} | Val {len(val_df):,}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 362,939 | Val 90,721\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "953769c5",
   "metadata": {},
   "source": [
    "# ASK TIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835cdf6",
   "metadata": {},
   "source": [
    "#### Train an XGBoost Ensemble with GroupKFolds based on the train set"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2ecc606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:14:29.904503Z",
     "start_time": "2025-05-20T20:14:29.902280Z"
    }
   },
   "source": "#TODO: DESCRIPTION",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "np.random.seed(42)\n",
    "# Define the parameter grid for tree sizes\n",
    "param_grid = {\n",
    "    'n_estimators': [2000],\n",
    "    'max_depth': [5],\n",
    "    'learning_rate': [0.05],\n",
    "    'subsample': [0.5],\n",
    "    #'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'scale_pos_weight': [3],\n",
    "    'gamma': [ 0.3],\n",
    "    'min_child_weight': [2],\n",
    "}\n",
    "\n",
    "search_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    eta=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    min_child_weight=2,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=60\n",
    ")\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=search_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=group_kfold,  # This will be replaced by GroupKFold below\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# To use group-based cross-validation, you need to pass a GroupKFold splitter to the cv argument:\n",
    "\n",
    "\n",
    "# grid_search.cv = group_kfold.split(X_train, y_train, groups=train_df['forest_id'])\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False, groups=train_df['forest_id'])\n",
    "\n",
    "# Get the best parameters and the corresponding F1 score\n",
    "best_params = grid_search.best_params_\n",
    "best_f1_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best F1 Score: {best_f1_score}\")\n",
    "# Perform prediction on the validation set\n",
    "y_pred = grid_search.best_estimator_.predict(X_val)\n",
    "\n",
    "# Calculate F1 score and confusion matrix\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(f\"F1 Score on Validation Set: {f1}\")\n",
    "print(f\"Confusion Matrix on Validation Set:\\n{cm}\")"
   ],
   "id": "14f12c1ee56ac514"
  },
  {
   "cell_type": "code",
   "id": "3f0290ccd90ec7ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:20:25.389767Z",
     "start_time": "2025-05-20T20:14:29.949434Z"
    }
   },
   "source": [
    "\n",
    "seeds = [42, 1337, 2025, 7, 99]\n",
    "base_params = dict(objective=\"binary:logistic\",\n",
    "                   eval_metric=\"aucpr\",\n",
    "                   eta=0.05,\n",
    "                   max_depth=7,\n",
    "                   subsample=0.8,\n",
    "                   colsample_bytree=0.7,\n",
    "                   min_child_weight=2,\n",
    "                   gamma=0.3,\n",
    "                   alpha=0.7,\n",
    "                   n_estimators=2000,\n",
    "                   tree_method=\"hist\",\n",
    "                   n_jobs=-1,\n",
    "                   early_stopping_rounds=60)\n",
    "\n",
    "groups = train_df[\"forest_id\"].values\n",
    "oof_proba = np.zeros(len(train_df))\n",
    "for fold, (i_tr, i_va) in enumerate(GroupKFold(5).split(X_train, y_train, groups), 1): # K fold based on forest_id\n",
    "    X_tr, y_tr = X_train[i_tr], y_train[i_tr]\n",
    "    X_va, y_va = X_train[i_va], y_train[i_va]\n",
    "    best, f1_b = np.zeros(len(i_va)), -1\n",
    "    \n",
    "\n",
    "    proba = np.zeros(len(i_va))\n",
    "    for s in seeds:\n",
    "        p = base_params | {\"random_state\": s,\n",
    "                            \"scale_pos_weight\": (y_tr == 0).sum() / (y_tr == 1).sum()}# adjust scaling of positive instances for the XGBoost model training based on the class imbalance\n",
    "        bst = XGBClassifier(**p).fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "        iso = CalibratedClassifierCV(bst, cv=\"prefit\", method=\"isotonic\").fit(X_va, y_va)# isotonic regression for better probability estimates\n",
    "        proba += iso.predict_proba(X_va)[:, 1]\n",
    "        proba /= len(seeds)\n",
    "        f1_tmp = f1_score(y_va, (proba >= 0.5))\n",
    "        if f1_tmp > f1_b: best, f1_b = proba, f1_tmp\n",
    "    oof_proba[i_va] = best\n",
    "prec, rec, thr = precision_recall_curve(y_train, oof_proba)\n",
    "tau_star = float(thr[np.argmax(2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-9))])\n",
    "print(f\"Optimal τ* = {tau_star:.3f}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal τ* = 0.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "fab2eea8",
   "metadata": {},
   "source": [
    "We choose the XGBoost Ensemble model because it made the best predictions on our validation set compared to other Models like XGBoost, RandomForest, ExtraTrees, Logistic Regression, KNeighrest Neighboor and SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032f3c4",
   "metadata": {},
   "source": [
    "#### Retrain xGboost Ensemble Model based on all original forests"
   ]
  },
  {
   "cell_type": "code",
   "id": "88c74bde33ce229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:31.095124Z",
     "start_time": "2025-05-20T20:20:25.434969Z"
    }
   },
   "source": [
    "full_X = feats[feature_cols].values\n",
    "full_y = feats[\"is_disturbance\"].values\n",
    "pos_w  = (full_y == 0).sum() / (full_y == 1).sum()\n",
    "\n",
    "ensemble = []\n",
    "for s in seeds:\n",
    "    params = base_params.copy()\n",
    "    params.update({\n",
    "        \"random_state\"          : s,\n",
    "        \"scale_pos_weight\"      : pos_w,\n",
    "        \"early_stopping_rounds\" : None\n",
    "    })\n",
    "\n",
    "    bst = XGBClassifier(**params).fit(full_X, full_y, verbose=False)\n",
    "\n",
    "    iso = CalibratedClassifierCV(bst, cv=\"prefit\", method=\"isotonic\")\n",
    "    iso.fit(full_X, full_y)\n",
    "\n",
    "    ensemble.append(iso)\n",
    "\n",
    "artifact = {\n",
    "    \"models\"      : ensemble,\n",
    "    \"threshold\"   : tau_star,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"params\"      : params,\n",
    "    \"timestamp\"   : datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with open(MODEL_DIR / \"xgb_iso_ensemble1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artifact, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"✓ Ensemble ({len(ensemble)} Modelle) gespeichert → models/xgb_iso_ensemble.pkl\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "/Users/tim.strauss/Library/Python/3.9/lib/python/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ensemble (5 Modelle) gespeichert → models/xgb_iso_ensemble.pkl\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "54ef0452d77de338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:25:31.199895Z",
     "start_time": "2025-05-20T20:25:31.141824Z"
    }
   },
   "source": [
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with open(MODEL_DIR / \"xgb_iso_ensemble1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artifact, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"✓ Ensemble ({len(ensemble)} Modelle) gespeichert → models/xgb_iso_ensemble.pkl\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ensemble (5 Modelle) gespeichert → models/xgb_iso_ensemble.pkl\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "7afdbbe4",
   "metadata": {},
   "source": [
    "#### Inferenz on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc90ed93ade748fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:26:05.119021Z",
     "start_time": "2025-05-20T20:25:31.211024Z"
    }
   },
   "source": [
    "TEST_CSV = \"data_6_channels_test_pub.csv\"\n",
    "OUT_CSV = \"data_6_channels_test_pub_with_predictions.csv\"\n",
    "MODEL_PKL = \"models/xgb_iso_ensemble.pkl\"\n",
    "\n",
    "# 6.1  Original-CSV laden (ohne Spaltenänderung merken)\n",
    "orig_test = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# 6.2  Für das Modell temporär umbenennen\n",
    "tmp = (orig_test\n",
    "       .rename(columns={\n",
    "    \"numerical_id\": \"forest_id\",\n",
    "    \"BLU\": \"blue\", \"GRN\": \"green\", \"RED\": \"red\",\n",
    "    \"NIR\": \"near_infrared\", \"SW1\": \"shortwave_infrared_1\",\n",
    "    \"SW2\": \"shortwave_infrared_2\"})\n",
    "       .copy())\n",
    "\n",
    "# 6.3  Feature-Engineering\n",
    "test_feat = (tmp.groupby(\"forest_id\")\n",
    "             .apply(engineer_features)\n",
    "             .fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "# 6.4  Ensemble + Threshold laden\n",
    "with open(MODEL_PKL, \"rb\") as f:\n",
    "    art = pickle.load(f)\n",
    "\n",
    "X_test = test_feat[art[\"feature_cols\"]].values\n",
    "proba = sum(m.predict_proba(X_test)[:, 1] for m in art[\"models\"]) / len(art[\"models\"])\n",
    "pred = (proba >= art[\"threshold\"]).astype(int)\n",
    "\n",
    "# 6.5  Prediction-DataFrame zum Mergen vorbereiten\n",
    "pred_df = (test_feat[[\"forest_id\", \"year\"]]\n",
    "           .assign(is_disturbance=pred)\n",
    "           .rename(columns={\"forest_id\": \"numerical_id\"}))\n",
    "\n",
    "# 6.6  Mit Original-CSV zusammenführen  (inner-merge garantiert 1-zu-1)\n",
    "merged = (orig_test\n",
    "          .merge(pred_df, on=[\"numerical_id\", \"year\"], how=\"left\"))\n",
    "\n",
    "# 6.7  Spaltenreihenfolge (wie Bild + neue Spalte)\n",
    "col_order = [\"fid\", \"year\", \"numerical_id\",\n",
    "             \"BLU\", \"GRN\", \"RED\", \"NIR\", \"SW1\", \"SW2\",\n",
    "             \"is_disturbance\"]\n",
    "merged = merged[col_order]\n",
    "\n",
    "# 6.8  Datei schreiben\n",
    "merged.to_csv(OUT_CSV, index=False)\n",
    "print(f\"✓ '{OUT_CSV}' geschrieben – {merged.shape[0]:,} Zeilen\")\n",
    "\n",
    "# 6.9  Kurzer Überblick\n",
    "print(f\"Label-Verteilung: 0 = {(merged.is_disturbance == 0).sum():,}  |  \"\n",
    "      f\"1 = {(merged.is_disturbance == 1).sum():,}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/s6j5yq4s12j6wvms4084h50h0000gn/T/ipykernel_98960/3781046846.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_feat = (tmp.groupby(\"forest_id\")\n",
      "/var/folders/9l/s6j5yq4s12j6wvms4084h50h0000gn/T/ipykernel_98960/3781046846.py:18: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_feat = (tmp.groupby(\"forest_id\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/xgb_iso_ensemble.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 24\u001B[0m\n\u001B[1;32m     18\u001B[0m test_feat \u001B[38;5;241m=\u001B[39m (tmp\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforest_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m              \u001B[38;5;241m.\u001B[39mapply(engineer_features)\n\u001B[1;32m     20\u001B[0m              \u001B[38;5;241m.\u001B[39mfillna(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mffill\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mfillna(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbfill\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m              \u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# 6.4  Ensemble + Threshold laden\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mMODEL_PKL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     25\u001B[0m     art \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m     27\u001B[0m X_test \u001B[38;5;241m=\u001B[39m test_feat[art[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_cols\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    308\u001B[0m     )\n\u001B[0;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'models/xgb_iso_ensemble.pkl'"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0925e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import random\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b946d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = \"data_6_channels_train.csv\" # ← Pfad ggf. anpassen\n",
    "df = (pd.read_csv(train_csv)\n",
    "      .rename(columns={\"numerical_id\": \"forest_id\",\n",
    "                       \"class\": \"is_disturbance\",\n",
    "                       \"BLU\": \"blue\", \"GRN\": \"green\", \"RED\": \"red\",\n",
    "                       \"NIR\": \"near_infrared\",\n",
    "                       \"SW1\": \"shortwave_infrared_1\", \"SW2\": \"shortwave_infrared_2\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebc8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(group: pd.DataFrame, lags: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Spektrale Indizes + Landsat-8 TCT + 3-Jahres-Stats + Lags/Deltas + Raum/Zeit.\"\"\"\n",
    "    eps = 1e-6\n",
    "    g = group.copy()\n",
    "\n",
    "    # ---------- Basis-Indizes ----------------------------------------\n",
    "    g[\"NDVI\"] = (g.near_infrared - g.red) / (g.near_infrared + g.red + eps)\n",
    "    g[\"NDMI\"] = (g.near_infrared - g.shortwave_infrared_1) / (g.near_infrared + g.shortwave_infrared_1 + eps)\n",
    "    g[\"NDWI\"] = (g.green - g.near_infrared) / (g.green + g.near_infrared + eps)\n",
    "    g[\"NBR\"] = (g.near_infrared - g.shortwave_infrared_2) / (g.near_infrared + g.shortwave_infrared_2 + eps)\n",
    "    g[\"EVI\"] = 2.5 * (g.near_infrared - g.red) / (g.near_infrared + 6 * g.red - 7.5 * g.blue + 1 + eps)\n",
    "    g[\"NBR2\"] = (g.shortwave_infrared_1 - g.shortwave_infrared_2) / (\n",
    "                g.shortwave_infrared_1 + g.shortwave_infrared_2 + eps)\n",
    "    \n",
    "    # ---------- Trockenheits-Ratio -----------------------------------\n",
    "    g[\"SWIR_ratio\"] = g.shortwave_infrared_2 / (g.shortwave_infrared_1 + eps)\n",
    "\n",
    "    # ---------- Landsat-8 Tasseled-Cap (Baig 2014) -------------------\n",
    "    b2, b3, b4 = g.blue, g.green, g.red\n",
    "    b5, b6, b7 = g.near_infrared, g.shortwave_infrared_1, g.shortwave_infrared_2\n",
    "    g[\"TCB\"] = 0.3029 * b2 + 0.2786 * b3 + 0.4733 * b4 + 0.5599 * b5 + 0.5080 * b6 + 0.1872 * b7\n",
    "    g[\"TCG\"] = -0.2941 * b2 - 0.2430 * b3 - 0.5424 * b4 + 0.7276 * b5 + 0.0713 * b6 - 0.1608 * b7\n",
    "    g[\"TCW\"] = 0.1511 * b2 + 0.1973 * b3 + 0.3283 * b4 + 0.3407 * b5 - 0.7117 * b6 - 0.4559 * b7\n",
    "    g[\"TCT4\"] = -0.8239 * b2 + 0.0849 * b3 + 0.4396 * b4 - 0.0580 * b5 + 0.2013 * b6 - 0.2773 * b7\n",
    "    g[\"TCT5\"] = -0.3294 * b2 + 0.0557 * b3 + 0.1056 * b4 + 0.1855 * b5 - 0.4349 * b6 + 0.8085 * b7\n",
    "    g[\"TCT6\"] = 0.1079 * b2 - 0.9023 * b3 + 0.4119 * b4 + 0.0575 * b5 - 0.0259 * b6 + 0.0252 * b7\n",
    "\n",
    "    # --- Interaktions-Features --------------------------------------\n",
    "    g[\"TCBxTCG\"] = g.TCB * g.TCG\n",
    "    g[\"TCBxTCW\"] = g.TCB * g.TCW\n",
    "    g[\"TCBxNBR\"] = g.TCB * g.NBR\n",
    "\n",
    "    # ---------- 3-Jahres-Median, Std, Anomalie (NDVI & NBR) ----------\n",
    "    for idx in (\"NDVI\", \"NBR\"):\n",
    "        g[f\"{idx}_med3\"] = g[idx].rolling(3, min_periods=2).median()\n",
    "        g[f\"{idx}_std3\"] = g[idx].rolling(3, min_periods=2).std()\n",
    "        g[f\"{idx}_anom\"] = (g[idx] - g[f\"{idx}_med3\"]) / (g[f\"{idx}_med3\"].abs() + eps)\n",
    "\n",
    "    # ---------- Lags & Deltas (blockweise, effizient) ---------------\n",
    "    base_cols = [\n",
    "        \"NDVI\", \"NDMI\", \"NDWI\", \"NBR\", \"EVI\", \"NBR2\", \"SWIR_ratio\",\n",
    "        \"TCB\", \"TCG\", \"TCW\", \"TCT4\", \"TCT5\", \"TCT6\",\n",
    "        \"TCBxTCG\", \"TCBxTCW\", \"TCBxNBR\",\n",
    "        \"blue\", \"green\", \"red\", \"near_infrared\", \"shortwave_infrared_1\", \"shortwave_infrared_2\"\n",
    "    ]\n",
    "\n",
    "    lag_features = []\n",
    "    for lag in range(1, lags + 1):\n",
    "        shifted = g[base_cols].shift(lag).rename(columns=lambda col: f\"l{lag}_{col}\")\n",
    "        deltas = (g[base_cols] - g[base_cols].shift(lag)).rename(columns=lambda col: f\"d{lag}_{col}\")\n",
    "        lag_features.extend([shifted, deltas])\n",
    "    g = pd.concat([g] + lag_features, axis=1)\n",
    "\n",
    "    # ---------- Raum- & Zeit-Features --------------------------------\n",
    "    g[\"year_sin\"] = np.sin(2 * np.pi * g.year / 10)\n",
    "    g[\"year_cos\"] = np.cos(2 * np.pi * g.year / 10)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e858194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft_file = \"all_data_all_features.csv\"\n",
    "ft_file = \"all_data_all_features_fast_approach.csv\"\n",
    "if not Path(ft_file).exists():\n",
    "    feats = (df.groupby(\"forest_id\")\n",
    "             .apply(engineer_features)\n",
    "             .fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "             .reset_index(drop=True))\n",
    "    feats.to_csv(ft_file, index=False)\n",
    "else:\n",
    "    feats = pd.read_csv(ft_file)\n",
    "\n",
    "feature_cols = [c for c in feats.columns\n",
    "                if c not in (\"forest_id\", \"year\", \"is_disturbance\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9e9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 362,939 | Val 90,721\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "correlations = feats.corr(numeric_only=True)[\"is_disturbance\"]\n",
    "features_to_drop = correlations[correlations.abs() < 0].index.tolist()\n",
    "\n",
    "#features_to_drop = ['is_disturbance', 'forest_id', 'fid', 'year', 'near_infrared', 'EVI', 'TCB', 'TCT4', 'TCBxTCG', 'NDVI_med3', 'NDVI_anom', 'NBR_med3', 'NBR_anom', 'l1_NDVI', 'l1_NDMI', 'l1_NDWI', 'l1_NBR', 'l1_EVI', 'l1_NBR2', 'l1_SWIR_ratio', 'l1_TCB', 'l1_TCG', 'l1_TCW', 'l1_TCT4', 'l1_TCT5', 'l1_TCT6', 'l1_TCBxTCG', 'l1_TCBxTCW', 'l1_TCBxNBR', 'l1_blue', 'l1_green', 'l1_red', 'l1_near_infrared', 'l1_shortwave_infrared_1', 'l1_shortwave_infrared_2', 'd1_EVI', 'd1_TCT4', 'd1_TCBxTCG', 'd1_near_infrared', 'l2_NDVI', 'l2_NDMI', 'l2_NDWI', 'l2_NBR', 'l2_EVI', 'l2_NBR2', 'l2_SWIR_ratio', 'l2_TCB', 'l2_TCG', 'l2_TCW', 'l2_TCT4', 'l2_TCT5', 'l2_TCT6', 'l2_TCBxTCG', 'l2_TCBxTCW', 'l2_TCBxNBR', 'l2_blue', 'l2_green', 'l2_red', 'l2_near_infrared', 'l2_shortwave_infrared_1', 'l2_shortwave_infrared_2', 'd2_EVI', 'd2_TCT4', 'd2_TCBxTCG', 'd2_near_infrared', 'l3_NDVI', 'l3_NDMI', 'l3_NDWI', 'l3_NBR', 'l3_EVI', 'l3_NBR2', 'l3_SWIR_ratio', 'l3_TCB', 'l3_TCG', 'l3_TCW', 'l3_TCT4', 'l3_TCT5', 'l3_TCT6', 'l3_TCBxTCG', 'l3_TCBxTCW', 'l3_TCBxNBR', 'l3_blue', 'l3_green', 'l3_red', 'l3_near_infrared', 'l3_shortwave_infrared_1', 'l3_shortwave_infrared_2', 'd3_EVI', 'd3_TCT4', 'd3_TCBxTCG', 'd3_near_infrared', 'year_sin', 'year_cos']\n",
    "feature_cols = [c for c in feats.columns\n",
    "                if c not in features_to_drop and c not in ('is_disturbance', 'forest_id', 'fid', 'year')]\n",
    "\n",
    "tr_idx, va_idx = next(gss.split(feats, groups=feats[\"forest_id\"]))\n",
    "\n",
    "train_df, val_df = feats.iloc[tr_idx], feats.iloc[va_idx]\n",
    "X_train, y_train = train_df[feature_cols].values, train_df[\"is_disturbance\"].values\n",
    "X_val, y_val = val_df[feature_cols].values, val_df[\"is_disturbance\"].values\n",
    "print(f\"Train {len(train_df):,} | Val {len(val_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_to_drop = ['is_disturbance', 'forest_id', 'fid', 'year', 'near_infrared', 'EVI', 'TCB', 'TCT4', 'TCBxTCG', 'NDVI_med3', 'NDVI_anom', 'NBR_med3', 'NBR_anom', 'l1_NDVI', 'l1_NDMI', 'l1_NDWI', 'l1_NBR', 'l1_EVI', 'l1_NBR2', 'l1_SWIR_ratio', 'l1_TCB', 'l1_TCG', 'l1_TCW', 'l1_TCT4', 'l1_TCT5', 'l1_TCT6', 'l1_TCBxTCG', 'l1_TCBxTCW', 'l1_TCBxNBR', 'l1_blue', 'l1_green', 'l1_red', 'l1_near_infrared', 'l1_shortwave_infrared_1', 'l1_shortwave_infrared_2', 'd1_EVI', 'd1_TCT4', 'd1_TCBxTCG', 'd1_near_infrared', 'l2_NDVI', 'l2_NDMI', 'l2_NDWI', 'l2_NBR', 'l2_EVI', 'l2_NBR2', 'l2_SWIR_ratio', 'l2_TCB', 'l2_TCG', 'l2_TCW', 'l2_TCT4', 'l2_TCT5', 'l2_TCT6', 'l2_TCBxTCG', 'l2_TCBxTCW', 'l2_TCBxNBR', 'l2_blue', 'l2_green', 'l2_red', 'l2_near_infrared', 'l2_shortwave_infrared_1', 'l2_shortwave_infrared_2', 'd2_EVI', 'd2_TCT4', 'd2_TCBxTCG', 'd2_near_infrared', 'l3_NDVI', 'l3_NDMI', 'l3_NDWI', 'l3_NBR', 'l3_EVI', 'l3_NBR2', 'l3_SWIR_ratio', 'l3_TCB', 'l3_TCG', 'l3_TCW', 'l3_TCT4', 'l3_TCT5', 'l3_TCT6', 'l3_TCBxTCG', 'l3_TCBxTCW', 'l3_TCBxNBR', 'l3_blue', 'l3_green', 'l3_red', 'l3_near_infrared', 'l3_shortwave_infrared_1', 'l3_shortwave_infrared_2', 'd3_EVI', 'd3_TCT4', 'd3_TCBxTCG', 'd3_near_infrared', 'year_sin', 'year_cos']\n",
    "\n",
    "# X_train = train_data.drop(columns=features_to_drop)\n",
    "# y_train = train_data['is_disturbance']\n",
    "\n",
    "\n",
    "# X_val = validation_data.drop(columns=features_to_drop)\n",
    "# y_val = validation_data['is_disturbance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb4385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf6a0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'gamma': 0.3, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 2000, 'scale_pos_weight': 3, 'subsample': 0.5}\n",
      "Best F1 Score: 0.5953464148162491\n",
      "F1 Score on Validation Set: 0.5879396984924623\n",
      "Confusion Matrix on Validation Set:\n",
      "[[90159   177]\n",
      " [  151   234]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "# Define the parameter grid for tree sizes\n",
    "param_grid = {\n",
    "    'n_estimators': [2000],\n",
    "    'max_depth': [5],\n",
    "    'learning_rate': [0.05],\n",
    "    'subsample': [0.5],\n",
    "    #'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'scale_pos_weight': [3],\n",
    "    'gamma': [ 0.3],\n",
    "    'min_child_weight': [2],\n",
    "}\n",
    "\n",
    "search_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    eta=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    min_child_weight=2,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=60\n",
    ")\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=search_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=group_kfold,  # This will be replaced by GroupKFold below\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# To use group-based cross-validation, you need to pass a GroupKFold splitter to the cv argument:\n",
    "\n",
    "\n",
    "# grid_search.cv = group_kfold.split(X_train, y_train, groups=train_df['forest_id'])\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False, groups=train_df['forest_id'])\n",
    "\n",
    "# Get the best parameters and the corresponding F1 score\n",
    "best_params = grid_search.best_params_\n",
    "best_f1_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best F1 Score: {best_f1_score}\")\n",
    "# Perform prediction on the validation set\n",
    "y_pred = grid_search.best_estimator_.predict(X_val)\n",
    "\n",
    "# Calculate F1 score and confusion matrix\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(f\"F1 Score on Validation Set: {f1}\")\n",
    "print(f\"Confusion Matrix on Validation Set:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot = [col for col in X_val.columns if col not in [\"year\", \"forest_id\"]]\n",
    "\n",
    "# Calculate how many rows we need (3 plots per row)\n",
    "num_features = len(features_to_plot)\n",
    "num_rows = int(np.ceil(num_features / 3))\n",
    "\n",
    "# Create subplots with calculated rows and 3 columns\n",
    "fig, axes = plt.subplots(num_rows, 3, figsize=(18, 6 * num_rows))\n",
    "axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    if i < len(axes):  # Make sure we don't exceed the number of available subplots\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot histograms on the current axis\n",
    "        ax.hist(\n",
    "            X_val[full_data[\"is_disturbance\"] == 0][feature].dropna(),\n",
    "            bins=30,\n",
    "            color='skyblue',\n",
    "            label='No Disturbance',\n",
    "            density=True\n",
    "        )\n",
    "        ax.hist(\n",
    "            full_data[full_data[\"is_disturbance\"] == 1][feature].dropna(),\n",
    "            bins=30,\n",
    "            color='red',\n",
    "            label='Disturbance',\n",
    "            alpha=0.2,\n",
    "            density=True\n",
    "        )\n",
    "        \n",
    "        # Add labels and title\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel(\"Percentage\")\n",
    "        ax.set_title(f\"{feature} distribution\")\n",
    "        ax.grid(False)\n",
    "        ax.legend()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "004aca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best Parameters for KNN: {'n_neighbors': 9}\n",
      "Best F1 Score for KNN: 0.4616615278113527\n",
      "F1 Score on Validation Set for KNN: 0.4288939051918736\n",
      "Confusion Matrix on Validation Set for KNN:\n",
      "[[135418    152]\n",
      " [   354    190]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for the number of neighbors\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': list(range(9, 11, 2))  # Odd numbers from 1 to 18\n",
    "}\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search_knn = GridSearchCV(\n",
    "    estimator=knn_model,\n",
    "    param_grid=param_grid_knn,\n",
    "    scoring='f1',\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding F1 score\n",
    "best_params_knn = grid_search_knn.best_params_\n",
    "best_f1_score_knn = grid_search_knn.best_score_\n",
    "\n",
    "print(f\"Best Parameters for KNN: {best_params_knn}\")\n",
    "print(f\"Best F1 Score for KNN: {best_f1_score_knn}\")\n",
    "# Perform prediction on the validation set using the best KNN model\n",
    "y_pred_knn = grid_search_knn.best_estimator_.predict(X_val)\n",
    "\n",
    "# Calculate F1 score and confusion matrix for KNN\n",
    "f1_knn = f1_score(y_val, y_pred_knn)\n",
    "cm_knn = confusion_matrix(y_val, y_pred_knn)\n",
    "\n",
    "print(f\"F1 Score on Validation Set for KNN: {f1_knn}\")\n",
    "print(f\"Confusion Matrix on Validation Set for KNN:\\n{cm_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96304034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
